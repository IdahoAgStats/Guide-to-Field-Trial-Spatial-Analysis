# rejected scripts

## Theory

### Spatial error model (SEM)

This is also refered to as the "moving average model" and is quite similar to auto-regressive model. SAR models the spatial dependence of the variable of interest. SEM models the spatial dependence of the error terms. 

$$\mathbf {Y= X\beta + u} \\ \mathbf {u = \lambda W + \nu}$$

Where $\mathbf{u}$ incorporates both the normally-distributed iid error and spatially correlated error. Like in SAR, $\mathbf{W}$ is a matrix of spatial weights and $\lambda$ is a parameter describing auto-correlation of the error terms. 

### ARIMA 

The AR1 and MA1 models can be combine into one: 

$$ y = X\beta + \rho W_1y+u \\ u = \lambda W_2 + \nu $$

## Applied Examples

### AR1xAR1 w/sommer)

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#library(sommer) 

Nin$colF <- as.factor(Nin$col)
Nin$rowF <- as.factor(Nin$row)

nin_ar1ar1 <- mmer(yield ~ gen,
                     random = ~ rep + 
                      vs(colF:rowF, Gu = kronecker(AR1(colF, rho=0.5), AR1(rowF, rho = 0.5), make.dimnames = TRUE)),
                   na.method.X = "include", na.method.Y = "exclude",
                   rcov = ~ units, data = Nin)

preds_ar1ar1 <- predict(nin_ar1ar1, classify = 'gen')$pvals 
```
This is not being used because the $\rho$ parameters are not estimated in the analysis. It's otherwise a very powerful and useful package and would be fine to use if the $\rho$ parameters were known in advance. 

### Spatial error model

(to use this, we need to define more neighbors)
```{r}
#library(spdep)
nin.sem <- errorsarlm(yield ~ gen + rep,
                       listw = nb2listw(xy.rook),
                       data = Nin, na.action = na.exclude)

mat.preds <- coef
```

### Combined spatial lag and error model (ARMA)
 
 (also uses the spdep package)
```{r}
nin.arma <- sacsarlm(yield ~ gen + rep,
                       listw = nb2listw(xy.rook),
                       data = Nin, na.action = na.exclude)


```

Obtaining predictions from the objects of `lagsarlm()`, `errorsarlm()`, and `sacsarlm()` is a challenging task. 

### spAMM

results comparable with nlme, but it's easier to just keep using nlme
```{r eval=FALSE, include=FALSE}
# from the package spAMM

nin.matern <- fitme(yield ~ gen + (1|rep) +
                      Matern(1|col.width + row.length),
                    fixed = list(rho = 1/range, nu = kappa),
                    data = Nin, method="ML")

preds = predict(nin.matern, re.form = ~gen,
                variance = list(fixefVar=TRUE))

matern.preds <- data.frame(gen = attributes(preds)$frame,
                           emmeans = preds,
                           var = attributes(preds)$fixefVar) %>%
  distinct() %>% mutate(SE = sqrt(var))
```


### rayshader exploration

(conclusion: it's very slow to render and the final plot underwhelming)
```{r eval=FALSE, include=FALSE}
#library(rayshader)
options(cores = 3)
```

```{r eval=FALSE, include=FALSE}
# this function is so slow!!
layout.gg <- ggplot(Nin, aes(x = row.length, y = col.width)) +
  geom_tile(aes(fill = yield), col = "black") +
  scale_fill_gradient(low = "white", high = "blue") +
  coord_fixed() + 
  theme_void() 

plot_gg(layout.gg, multicore = T, width = 10, height = 5, scale = 300, 
        background = "white",shadowcolor = "#3a4f70") 

render_snapshot(clear = T)  # for rayshader, so so slow
```
  
  
## Spatial Autoregression

These models are not being used further because:
1. They are only set up for CRD and RCBD
2. the blocks AND genotypes must be coded as numeric vectors 
3. This uses OLS to solve the equations
4. In order to get out any means, many comparisons have to be made rather than just returning the means 
5. the shiny app seems to have some problems - it did not load the NIN data set properly (but that did load fine into R with read.csv())

(also called the Spatial lag model or SAR)
This requires the package **spANOVA** which is large and may take a few seconds to load. 
```{r, message=FALSE, warning=FALSE, include=FALSE}
#library(spANOVA)
## get that model in here!!

#library(agridat); data("stroup.nin")
nin_sar <- aovSar.rcbd(resp = as.vector(stroup.nin$yield), 
                                 treat = as.vector(stroup.nin$gen), 
                                 block = as.vector(stroup.nin$rep), 
                                 coord = cbind(stroup.nin$col, stroup.nin$row))
```
                    
## Traits with non-gaussian distributions

(decided to cut this and consider building a separate guide for GLMMs in the future)

Here is where things start to get a bit crazy. Generalized linear models with spatial covariates are more difficult to fit and hence that functionality is not available in **nlme** except for log-normal models. **lme4** can still fit the latin square model using a spat of link functions. 

### Count traits: 

Count traits are discrete rather than continuous and often follow a poisson distribution or in the case of over-dispersed data, a negative binomial distribution. 

The *cochran.wireworms* data set reports wireworms counts after a fumigation treatment. There are 5 treatment levels, "K", "M", "N", "O", and "P", and 5 replicates. It is arranged in a latin square. 
```{r}
data(cochran.wireworms)
stem(cochran.wireworms$worms)

ggdesplot(cochran.wireworms, worms ~ col*row,
        text=trt, cex=1, col.regions = vir, 
        xlab = "col", ylab = "row",
        main="cochran wireworms")

cochran.wireworms <- transform(cochran.wireworms, colf = as.factor(col), rowf = as.factor(row))

m_count1 <- glmer(worms ~ trt + (1|colf) + (1|rowf),
                  family = poisson(link = "log"),
                  data = cochran.wireworms)
```

This warning message `boundary (singular) fit: see ?isSingular` indicates the one of the random effects is at the edge of parameter space, or in other words, near zero. Let's check it out:

```{r}
VarCorr(m_count1)
```
Since `colf` is effectively zero, the model could be refit without it. 

### Another Count Example

Here's an example using a RCB design, *cochran.eelworms*, which looks at eelworms counts before and after different fumigation treatments. 
```{r}
data(cochran.eelworms)

desplot(cochran.eelworms, initial ~ col*row,
        flip=TRUE, col.regions = vir, 
        main="cochran eelworms")

# might need inla for this, or gams
```


### Percent variables 

These are variables that are bounded between 0 and 100 (e.g. percent germination, percent infection, percent survival). These can behave like continuous variables, especially when they are not reported only in discrete increments (e.g. 10, 20, 30). Percent variables heavily skewed towards their bounds of 0 and 100 are difficult to model. Percent variables that exceed 100 (e.g. relative yield expressed as a percentage) can quite often be treated as continuous. But, as always, it depends on the actual distribution of the variable.

If the data are bound between zero and 100 and have no values exactly equal to zero or one, the beta distribution can be used on proportion data, which is percentage divided by 100. 

The example below reuses a data set loaded earlier, *cochran.crd*, that is looking at percent infection of scab among potatoes clones after sulfur treatments of varying concentrations.  
```{r}
data(cochran.crd)

range(cochran.crd$inf)
hist(cochran.crd$inf)

# needs gams
# library(mgcv)
# mod <- gam(inf ~ x1 + x2 + s(latitude, longitude, bs = "gp", m = 2), # change m
#            family = betar(link='logit'), 
#            data = data)
```

### Ordinal variables 
(e.g. disease scores)

```{r}
data(lee.potatoblight)
```

### Binomial traits
(e.g. )
```{r}
data(gotway.hessianfly)
```
