# Comparing Spatial Models {#compare-model}

Now that we've built all these spatials models, how do we pick the right one? 

First, assemble all the model objects into one list. They are generated from different processes, as shown by the `class` attribute of each one. 
```{r message=FALSE, warning=FALSE}
library(purrr); library(dplyr)
all.models <- mget(ls(pattern = "^nin.*"))
# print out their class
map(all.models, class)
```

## Spatial dependence of residuals

It would be helpful to know if these methods were effective in reducing the spatial dependence among the error residuals. 

The function below extracts the residuals from each model and is needed because of wonky handling of NA values by the packages **sommer** and **SpATS**. 

```{r message=FALSE, warning=FALSE}
L1 <- nrow(Nin)
non_na <- !is.na(Nin$yield)
L2 <- sum(non_na)

residuals <- map(all.models, function (x) {
  
  resids <- residuals(x)
  
  if(is.data.frame(resids)) {
    colnum = ncol(resids)
    resids = resids[,colnum]
  }
  
  if(length(resids) == L2) {
    resids_pl = rep(NA, L1)
    resids_pl[non_na] = resids
    resids = resids_pl
  }
  
  return(resids)
})
```

Run Global Moran's I test on the extracted residuals:

```{r message=FALSE, warning=FALSE}
library(spdep)

xy.rook <- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type="rook")

Moran.I <- map_df(residuals, function(x) {
  mi = moran.test(x, nb2listw(xy.rook), na.action = na.exclude)
  mi.stat <- mi$estimate
  mi.stat$p.value <- mi$p.value
  return(mi.stat)
}) %>% mutate(model = names(all.models)) %>% dplyr::select(c(5, 1:4)) %>% 
  mutate_at(2:5, round, 4) %>% arrange(p.value)

Moran.I
```

Only one model, `nin.spline` resulted in an improvement in Moran's I. Nearest neighbor approaches can also improve Moran's I. The significant p-values indicate that auto-correlation is still present in those models. However, that doesn't mean the other models are ineffective. The other models incorporate the spatial auto-correlation directly into the error terms. 

## Compare log likelihood

Since these are not nested models, likelihood ratio tests cannot be performed. Log likelihood can be compared within the models from **nlme** but not across packages since they use different estimation procedures.

```{r}
nlme.mods <- list(nin.lme, nin.exp, nin.gaus, nin.sph, nin.pow, nin.matern)

names(nlme.mods) <- c(c("lme", "exponential", "gaussian", 
                        "spherical", "power", "matern"))

lapply(nlme.mods, logLik)
```

Larger log likelihoods indicate a better fitting model to the data. A rule of thumb when comparing log likelihoods is that differences less than 2 are not considered notable. These results suggest that the Gaussian, spherical, power and Matérn models are substantially equivalent in capturing the variation present in this data set. 

## Compare changes in error

```{r}
exp.error <- lapply(nlme.mods, sigma)
exp.error
```

The overall experimental error, $\sigma$, increased in the correlated error models because field variation has been re-partitioned to the error when it was (erroneously) absorbed by the other experimental effects. 

As a result, the coefficient of variation is not a good metric for evaluating the quality of spatial models. 

```{r}
CV = lapply(nlme.mods, function(x) {
  sigma(x)/mean(fitted(x), na.rm = T) * 100
})
CV
```

## Experimental power

Simulation studies indicate that incorporating spatial correlation into field trial analysis can improve the overall power of the experiment (the probability of detecting true differences in treatments). When working with data from an experiment, power is a transformed p-value. Performing ANOVA can indicate which approach maximizes power. 

```{r message=FALSE, warning=FALSE}
anovas <- lapply(nlme.mods, function(x){ 
  aov <- as.data.frame(anova(x))[2,]
  })

bind_rows(anovas) %>% mutate(model = c("lme", "exponential", "gaussian", 
                                            "spherical", "power", "matern")) %>% 
  arrange(desc(`p-value`)) %>% dplyr::select(c(5, 1:4))
```

This table indicates changes in the hypothesis test for "gen".  There is a dramatic change in power for this test when incorporating spatial covariance structures. 

## Standard error of variety estimates

Retrieve predictions generated in the previous section:
```{r}
#(standardise names for downstream merging step)
preds.ar1ar1 <- preds.ar1ar1  %>% rename(emmean = "predicted.value", SE = "standard.error") 
all.preds <- mget(ls(pattern = "^preds.*"))
```

Extract standard errors and plot: 
```{r}
errors <- lapply(all.preds, "[", "SE")
pred.names <- gsub("preds.", "", names(errors))
error_df <- bind_cols(errors)
colnames(error_df) <- pred.names
```

```{r SE-box-fig, echo=FALSE, fig.cap='Differences in Variety Standard Error', out.width='80%', fig.asp=0.75, fig.align='center'}
boxplot(error_df, ylab = "standard errors", xlab = "linear model", col = "dodgerblue3")
```

## Compare variety estimates

Extract estimates:
```{r}
preds <- lapply(all.preds, "[", "emmean")
preds_df <- bind_cols(preds)
colnames(preds_df) <- pred.names
preds_df$gen <- preds.exp$gen
```

Plot changes in ranks: 

```{r gen-ranks-fig, echo=FALSE, fig.align='center', fig.asp=0.75, fig.cap='Differences in Variety Ranks', message=FALSE, warning=FALSE, out.width='85%'}
library(ggplot2); library(reshape2)

lev <- c("lme", "exp", "gaus", "mat", "pow", "sph", "spline", "ar1ar1")

melt(preds_df, id.vars = "gen", variable.name = "model", value.name = "emmeans") %>% 
  mutate(model = factor(model, levels = lev)) %>% 
  ggplot(aes(x = model, y = emmeans, group = gen)) +
  geom_point() +
  geom_line() +
  ylab("yield means for gen") + 
  theme_minimal()
```

The black lines link the least squares means for a single variety. There is some consistency in the rankings between exponential, Gaussian, power, Matérn, and spherical covariance models. The control RCBD model, "lme", has fundamentally different rankings. The spline and AR1xAR1 ranking are also sightly different from the other models. 

Nevertheless, the following plot indicates considerable consensus in the least squares means from all of the spatial models. The upper diagonal contains Pearson correlations between those values. 

```{r ls-panel-fig, echo=FALSE, fig.align='center', fig.asp=1, fig.cap='Correlations in Variety Means', message=FALSE, warning=FALSE, out.width='90%'}
library(psych)
pairs.panels(preds_df[,-9], smooth = F, density = F, ellipses = F, 
             hist.col = "gold", pch = 1)
```

## Making decisions

There is no consensus on how to pick the best model. Some studies rely on log likelihood, while others seek to maximize the experimental power. Others have sought to minimize the root mean square error from cross validation.

The evidence suggest that for this data set, using any spatial model is better than running a naïve RCBD model.  
